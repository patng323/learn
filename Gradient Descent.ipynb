{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent (Linear Regression and Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genData(size, noise_mean = None, noise_sd = None):\n",
    "    x1 = np.random.uniform(10, 20, size)\n",
    "    x2 = np.random.uniform(1,5, size)\n",
    "\n",
    "    # True formula: y = 2 + x1*5 + x2*4 + noise\n",
    "    true_weights = [2, 5, 4]\n",
    "\n",
    "    # Add the \"x_0 = 1\" (called the intercept term) so that we can use matrix multiplication\n",
    "    X = np.vstack((np.ones(size), x1, x2))\n",
    "\n",
    "    y = np.dot(true_weights, X)\n",
    "\n",
    "    if noise_mean is not None and noise_sd is not None:\n",
    "        noise = np.random.normal(noise_mean, noise_sd, size)\n",
    "        y = y + noise\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some background stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True formula: y = 2 + x1*5 + x2*4 + noise (if needed)\n",
      "\n",
      "X: each column is an x vector\n",
      "[[  1.           1.           1.           1.           1.           1.\n",
      "    1.           1.           1.           1.        ]\n",
      " [ 17.91083053  11.95308736  14.26630328  19.91906186  14.1198285\n",
      "   14.30245984  14.09699088  18.75599415  15.35029217  18.17591462]\n",
      " [  4.67970815   2.46769548   2.43270842   1.37310306   2.64425863\n",
      "    4.86233748   2.32753645   2.28021862   2.74270834   4.54512193]]\n",
      "\n",
      "Y: each element is a y value\n",
      "[ 110.27298524   71.63621874   83.06235008  107.08772155   83.17617701\n",
      "   92.96164913   81.79510018  104.90084523   89.72229419  111.06006082]\n"
     ]
    }
   ],
   "source": [
    "size = 10\n",
    "X, y = genData(size)\n",
    "print \"True formula: y = 2 + x1*5 + x2*4 + noise (if needed)\"\n",
    "print\n",
    "print \"X: each column is an x vector\"\n",
    "print X\n",
    "print\n",
    "print \"Y: each element is a y value\"\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost function\n",
    "# Let h_θ(x) be the predicting function: h_θ(x) = w0 + w1*x1 + w2*x2\n",
    "# where θ = [w0, w1, w2]\n",
    "# For OLS, cost function: J(θ) = 1/2 sum_over_all_X_y_pairs((h_θ(xi) - yi)^2)\n",
    "\n",
    "# We want to choose θ so as to minimize J(θ)\n",
    "\n",
    "# Algorithm:\n",
    "# Starts with some “initial guess” for θ\n",
    "# Repeat:\n",
    "#     Changes θ to make J(θ) smaller:\n",
    "#        θj := θj - alpha * gradient_of_J_wrt_θj \n",
    "#        where j is the j-th component of θ. (θ0 = w0, θ1 = w1, θ2 = w2)  \n",
    "#     Reminder:\n",
    "#     - If the gradient is -ve, we're going downslope, we want to move θ forward. \n",
    "#       'Minus a negative number' means add, and thus increasing θ.\n",
    "# until we converge to a value of θ that minimizes J(θ)\n",
    "#\n",
    "# Convergence check usually is: L2(gradient) < A_stopping_criteria\n",
    "# http://stackoverflow.com/questions/13059564/what-should-be-a-generic-enough-convergence-criteria-of-stochastic-gradient-desc\n",
    "\n",
    "# If we only have ONE single training example:\n",
    "#\n",
    "# Gradient of J wrt θj = d J(θ) / d θj = \n",
    "# d 1/2 sum_over_all_X_y_pairs((h_θ(xi) - yi)^2)/ d θj = \n",
    "# (h_θ(x) - y)x_j\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.01\n",
    "w = [0,0,0] # Initialize the weight\n",
    "\n",
    "# h_θ(x) for all training examples\n",
    "# Example format of yhats: [  99.49519886  111.03223123  109.21172544  104.2711384    90.53712043]\n",
    "yhats = np.dot(w, X)\n",
    "ydiffs = yhats - y # (h_θ(x) - y) for all training examples\n",
    "\n",
    "# (h_θ(x) - y)x0 all training examples\n",
    "# This is the adjustment for w0\n",
    "g0 = ydiffs * X[0,:]\n",
    "g1 = ydiffs * X[1,:]\n",
    "g2 = ydiffs * X[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for w0 (1st row), w1 and w2 (last row) for ALL examples.\n",
      "Each column represent an example.  Five in total.\n",
      "[ -99.49519886 -111.03223123 -109.21172544 -104.2711384   -90.53712043]\n",
      "[-1540.99507572 -1904.41302337 -1750.40553776 -1610.86728187 -1288.93606021]\n",
      "[-356.54865175 -305.37736894 -498.77999791 -506.19999069 -229.39988431]\n"
     ]
    }
   ],
   "source": [
    "print \"Gradient for w0 (1st row), w1 and w2 (last row) for ALL examples.\"\n",
    "print \"Each column represent an example.  Five in total.\"\n",
    "print g0\n",
    "print g1\n",
    "print g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for ALL examples.\n",
      "Each row represent a component in the weights (e.g. w0 = 1st row)\n",
      "Each column represent an example.\n",
      "\n",
      "[[  -99.49519886  -111.03223123  -109.21172544  -104.2711384    -90.53712043]\n",
      " [-1540.99507572 -1904.41302337 -1750.40553776 -1610.86728187\n",
      "  -1288.93606021]\n",
      " [ -356.54865175  -305.37736894  -498.77999791  -506.19999069\n",
      "   -229.39988431]]\n"
     ]
    }
   ],
   "source": [
    "# This will give the same result\n",
    "g = ydiffs * X\n",
    "print \"Gradient for ALL examples.\"\n",
    "print \"Each row represent a component in the weights (e.g. w0 = 1st row)\"\n",
    "print \"Each column represent an example.\"\n",
    "print\n",
    "print g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (Again)\n",
    "# Algorithm:\n",
    "# Starts with some “initial guess” for θ\n",
    "# Repeat:\n",
    "#     Changes θ to make J(θ) smaller:\n",
    "#        θj := θj - alpha * gradient_of_J_wrt_θj \n",
    "#        where j is the j-th component of θ. (θ0 = w0, θ1 = w1, θ2 = w2)  \n",
    "# until we converge to a value of θ that minimizes J(θ)\n",
    "\n",
    "# If we only have ONE single training example:\n",
    "#\n",
    "# Gradient_of_J_wrt_θj = d J(θ) / d θj = \n",
    "# d 1/2 sum_over_all_X_y_pairs((h_θ(xi) - yi)^2)/ d θj = \n",
    "# (h_θ(x) - y)x_j\n",
    "\n",
    "# If we have many training example:\n",
    "#\n",
    "# Gradient_of_J_wrt_θj = alpha * sum_over_all_examples((h_θ(x) - y)x_j) / number_of_examples\n",
    "#\n",
    "# Note: we have to average it out over the examples, otherwise gradient will increase as sample size increases.\n",
    "g_sum_over_examples = np.sum(g, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each column represents the sum of gradient from all examples.  1st column means w0.\n",
      "[-0.3942039   0.79810913 -4.0575618 ]\n"
     ]
    }
   ],
   "source": [
    "print \"Each column represents the sum of gradient from all examples.  1st column means w0.\"\n",
    "print g_sum_over_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wdelta = -1 * alpha * g_sum_over_examples / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.88407806e-05  -1.59621825e-04   8.11512360e-04]\n"
     ]
    }
   ],
   "source": [
    "print wdelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = w + wdelta\n",
    "print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True formula: y = 2 + x1*5 + x2*4 + noise\n",
      "After 1000  iterations, w =  [ 0.54728746  5.18380913  3.52768918]\n"
     ]
    }
   ],
   "source": [
    "# Let's put everything under a loop\n",
    "size = 1000\n",
    "X, y = genData(size, noise_mean = 0, noise_sd = 0.5)\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.001\n",
    "stopping_critera = 0.0001\n",
    "w = [0,0,0] # Initialize the weight\n",
    "iterations = 1000\n",
    "\n",
    "for i in range(iterations):\n",
    "    yhats = np.dot(w, X)\n",
    "    ydiffs = yhats - y # (h_θ(x) - y) for all training examples\n",
    "    g = ydiffs * X\n",
    "    g_sum_over_examples = np.sum(g, axis=1)\n",
    "    wdelta = -1 * alpha * g_sum_over_examples / size\n",
    "    \n",
    "    # Convergence check\n",
    "    if np.linalg.norm(wdelta) <= stopping_critera:\n",
    "        break\n",
    "        \n",
    "    #print \"Iteration\", i, \"- wdelta =\", wdelta\n",
    "    w = w + wdelta\n",
    "\n",
    "print \"True formula: y = 2 + x1*5 + x2*4 + noise\"\n",
    "print \"After\", i+1, \" iterations, w = \", w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression using Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Batch Gradient Descent, in each loop we update w by batching up the changes from all the examples.\n",
    "- In Stochastic Gradient Descent, within each loop, we loop through all examples, and update w for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True formula: y = 2 + x1*5 + x2*4 + noise\n",
      "After 500  iterations, w =  [ 2.05590869  4.9859899   4.00956634]\n"
     ]
    }
   ],
   "source": [
    "# Let's put everything under a loop\n",
    "size = 1000\n",
    "X, y = genData(size, noise_mean = 0, noise_sd = 0.2)\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.005\n",
    "stopping_critera = 0.0001\n",
    "w = [0,0,0] # Initialize the weight\n",
    "iterations = 500\n",
    "\n",
    "for i in range(iterations):\n",
    "    for m in range(size):\n",
    "        x = X[:,m] # a single example\n",
    "        y_single = y[m]\n",
    "        \n",
    "        # (h_θ(x) - y) for one training example\n",
    "        yhat = np.dot(w, x) \n",
    "        ydiff = yhat - y_single\n",
    "        \n",
    "        # Gradient for one example        \n",
    "        g = ydiff * x\n",
    "        \n",
    "        wdelta = -1 * alpha * g\n",
    "    \n",
    "        # Convergence check\n",
    "        if np.linalg.norm(wdelta) <= stopping_critera:\n",
    "            break\n",
    "        \n",
    "        w = w + wdelta\n",
    "\n",
    "print \"True formula: y = 2 + x1*5 + x2*4 + noise\"\n",
    "print \"After\", i+1, \" iterations, w = \", w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Linear Regression using Batch Gradient Descent with Spark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True formula: y = 2 + x1*5 + x2*4 + noise\n",
      "After 500  iterations, w =  [ 0.63756532  5.08123991  4.03859815]\n"
     ]
    }
   ],
   "source": [
    "# Let's put everything under a loop\n",
    "size = 1000\n",
    "X, y = genData(size, noise_mean = 0, noise_sd = 0.5)\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.005\n",
    "stopping_critera = 0.0001\n",
    "w = [0,0,0] # Initialize the weight\n",
    "iterations = 500\n",
    "\n",
    "# Convert X, y into an array of Point object.  Note that each column in X is an x, so we\n",
    "# need to do the transpose (X.T), otherwise we'll be getting each row of X instead of each column.\n",
    "points = [Point(one_x, one_y) for one_x, one_y in zip(X.T, y)]\n",
    "data = sc.parallelize(points).cache()\n",
    "\n",
    "for i in range(iterations):\n",
    "    # If we don't use broadcast, the driver has to send out w for each task.\n",
    "    wbroadcast = sc.broadcast(w)\n",
    "\n",
    "    # Run this for each single example to get the partial gradient\n",
    "    # (h_θ(x) - y)x_j\n",
    "    partial_gradients = data.map(lambda point: (np.dot(wbroadcast.value, point.x)- point.y)*point.x)\n",
    "    \n",
    "    # Add up the partial gradient from all the example, and average them out.\n",
    "    g = partial_gradients.reduce(lambda a, b: a + b)/size\n",
    "    \n",
    "    wdelta = -1 * alpha * g\n",
    "    \n",
    "    # Convergence check\n",
    "    if np.linalg.norm(wdelta) <= stopping_critera:\n",
    "        break\n",
    "        \n",
    "    #print \"Iteration\", i, \"- wdelta =\", wdelta\n",
    "    w = w + wdelta\n",
    "\n",
    "print \"True formula: y = 2 + x1*5 + x2*4 + noise\"\n",
    "print \"After\", i+1, \" iterations, w = \", w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "```\n",
    "- Let's say we have input x and outcome y, where y is a classification, either 0 or 1. \n",
    "- We want to have a function h_θ(x) = Pr(y = 1 | x; θ)\n",
    "\n",
    "- Options for h_θ(x) (remember: h_θ(x) is a probability):\n",
    "  - h_θ(x) = θ'X (i.e. h_θ(x) = w0 + w1x1 + w2x2 + ...)\n",
    "    - Problem: the range of RHS is -inf to +inf\n",
    "    \n",
    "  - ln(h_θ(x)) = θ'X \n",
    "    - Problem: 0 <= h_θ(x) <= 1; so -inf < ln(h_θ(x)) < 0. The range of LHS != range of RHS\n",
    "    \n",
    "  - ln(h_θ(x) / 1 - h_θ(x)) = θ'X\n",
    "    - Good.  If h_θ(x) = 0, LHS = -inf.  If h_θ(x) = 1, LHS = +inf.\n",
    "    - So the range of LHS = range of RHS\n",
    "\n",
    " -  Take exponent of both sides:\n",
    "   - exp(ln(h_θ(x) / 1 - h_θ(x))) = exp(θ'X) ==> \n",
    "   - h_θ(x) = 1 / (1 + exp(-θ'X))\n",
    "   \n",
    " - Prediction:\n",
    "   - y_hat = 1 if h_θ(x) >= 0.5\n",
    "   - y_hat = 0 if h_θ(x) < 0.5\n",
    "   \n",
    " - To solve for θ, we use Gradient Descent.\n",
    "   - To use GD, we need a cost function.  \n",
    "   - Cost = high if we make a mistake: y_hat = 0 if actual y = 1; or y_hat = 0 if actual y = 1\n",
    "   - J(θ) = -y ln(h_θ(x)) - (1-y)lb(h_θ(x))\n",
    "     - E.g. if y = 1, but h_θ(x) = 0.1 (predict y = 0), Cost = -1 * ln(0.1) = -1 * -2.3 = 2.3\n",
    "     -      if y = 1, but h_θ(x) = 0.9 (predict y = 1), Cost = -1 * ln(0.9) = -1 * -0.1 = 0.1\n",
    " \n",
    "   - Gradient of J(θ) at each j-th compoent = sum_over_examples[(h_θ(xi) - yi)x_ij]\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genData(size, noise_mean = None, noise_sd = None):\n",
    "    x1 = np.random.uniform(10, 20, size)\n",
    "    x2 = np.random.uniform(1,5, size)\n",
    "\n",
    "    # True formula: y = 2 + x1*5 + x2*4 + noise\n",
    "    true_weights = [2, 5, 4]\n",
    "\n",
    "    # Add the \"x_0 = 1\" (called the intercept term) so that we can use matrix multiplication\n",
    "    X = np.vstack((np.ones(size), x1, x2))\n",
    "\n",
    "    y = np.dot(true_weights, X)\n",
    "\n",
    "    if noise_mean is not None and noise_sd is not None:\n",
    "        noise = np.random.normal(noise_mean, noise_sd, size)\n",
    "        y = y + noise\n",
    "            \n",
    "    return X, (y > 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = 500\n",
    "X, y = genData(size)\n",
    "train_len = int(size * 0.8)\n",
    "\n",
    "X_train = X.T[:train_len,:]\n",
    "y_train = y.T[:train_len].astype(int)\n",
    "\n",
    "X_test = X.T[train_len:,:]\n",
    "y_test = y.T[train_len:].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50000  iterations, w =  [-11.87490136   0.78171116   0.24834292]\n"
     ]
    }
   ],
   "source": [
    "# Learning rate\n",
    "alpha = 0.01\n",
    "stopping_critera = 0.0001\n",
    "w = [0,0,0] # Initialize the weight\n",
    "iterations = 50000\n",
    "\n",
    "def h(w, x):\n",
    "    return (1 / (1 + np.exp(-1 * np.dot(w,x.T))))\n",
    "            \n",
    "for i in range(iterations):\n",
    "    #  - Gradient of J(θ) at each j-th compoent = sum_over_examples[(h_θ(xi) - yi)x_ij]\n",
    "    #    - h_θ(x) = 1 / (1 + exp(-θ'X))\n",
    "    g = np.dot(h(w, X_train)-y_train.reshape((1,-1)), X_train).flatten()\n",
    "    wdelta = -1 * alpha * g / size\n",
    "    \n",
    "    # Convergence check\n",
    "    if np.linalg.norm(wdelta) <= stopping_critera:\n",
    "        break\n",
    "        \n",
    "    #print \"Iteration\", i, \"- wdelta =\", wdelta\n",
    "    w = w + wdelta\n",
    "\n",
    "print \"After\", i+1, \" iterations, w = \", w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision from sklearn LogisticRegression = 0.91\n"
     ]
    }
   ],
   "source": [
    "y_hat = (h(w, X_test) >= 0.5).astype(int)\n",
    "print \"Precision from sklearn LogisticRegression =\", np.average(y_test == y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision from sklearn LogisticRegression = 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mod = LogisticRegression()\n",
    "mod = mod.fit(X_train, y_train)\n",
    "y_predicted = mod.predict(X_test)\n",
    "print \"Precision from sklearn LogisticRegression =\", np.average(y_test == y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
