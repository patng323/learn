{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genData(size, noise_mean = None, noise_sd = None):\n",
    "    x1 = np.random.uniform(10, 20, size)\n",
    "    x2 = np.random.uniform(1,5, size)\n",
    "\n",
    "    # True formula: y = 2 + x1*5 + x2*4 + noise\n",
    "    true_weights = [2, 5, 4]\n",
    "\n",
    "    # Add the \"x_0 = 1\" (called the intercept term) so that we can use matrix multiplication\n",
    "    X = np.vstack((np.ones(size), x1, x2))\n",
    "\n",
    "    y = np.dot(true_weights, X)\n",
    "\n",
    "    if noise_mean is not None and noise_sd is not None:\n",
    "        noise = np.random.normal(noise_mean, noise_sd, size)\n",
    "        y = y + noise\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some background stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True formula: y = 2 + x1*5 + x2*4 + noise (if needed)\n",
      "\n",
      "X: each column is an x vector\n",
      "[[  1.           1.           1.           1.           1.        ]\n",
      " [ 13.13127207  14.48761198  16.08142012  10.72382187  12.89606835]\n",
      " [  4.33099652   2.42015041   2.22872919   4.70708422   3.54216482]]\n",
      "\n",
      "Y: each element is a y value\n",
      "[ 84.9803464   84.11866154  91.32201737  74.44744624  80.64900105]\n"
     ]
    }
   ],
   "source": [
    "size = 5\n",
    "X, y = genData(size)\n",
    "print \"True formula: y = 2 + x1*5 + x2*4 + noise (if needed)\"\n",
    "print\n",
    "print \"X: each column is an x vector\"\n",
    "print X\n",
    "print\n",
    "print \"Y: each element is a y value\"\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost function\n",
    "# Let h_θ(x) be the predicting function: h_θ(x) = w0 + w1*x1 + w2*x2\n",
    "# where θ = [w0, w1, w2]\n",
    "# For OLS, cost function: J(θ) = 1/2 sum_over_all_X_y_pairs((h_θ(xi) - yi)^2)\n",
    "\n",
    "# We want to choose θ so as to minimize J(θ)\n",
    "\n",
    "# Algorithm:\n",
    "# Starts with some “initial guess” for θ\n",
    "# Repeat:\n",
    "#     Changes θ to make J(θ) smaller:\n",
    "#        θj := θj - alpha * gradient_of_J_wrt_θj \n",
    "#        where j is the j-th component of θ. (θ0 = w0, θ1 = w1, θ2 = w2)  \n",
    "#     Reminder:\n",
    "#     - If the gradient is -ve, we're going downslope, we want to move θ forward. \n",
    "#       'Minus a negative number' means add, and thus increasing θ.\n",
    "# until we converge to a value of θ that minimizes J(θ)\n",
    "#\n",
    "# Convergence check usually is: L2(gradient) < A_stopping_criteria\n",
    "# http://stackoverflow.com/questions/13059564/what-should-be-a-generic-enough-convergence-criteria-of-stochastic-gradient-desc\n",
    "\n",
    "# If we only have ONE single training example:\n",
    "#\n",
    "# Gradient of J wrt θj = d J(θ) / d θj = \n",
    "# d 1/2 sum_over_all_X_y_pairs((h_θ(xi) - yi)^2)/ d θj = \n",
    "# (h_θ(x) - y)x_j\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.01\n",
    "w = [0,0,0] # Initialize the weight\n",
    "\n",
    "# h_θ(x) for all training examples\n",
    "# Example format of yhats: [  99.49519886  111.03223123  109.21172544  104.2711384    90.53712043]\n",
    "yhats = np.dot(w, X)\n",
    "ydiffs = yhats - y # (h_θ(x) - y) for all training examples\n",
    "\n",
    "# (h_θ(x) - y)x0 all training examples\n",
    "# This is the adjustment for w0\n",
    "g0 = ydiffs * X[0,:]\n",
    "g1 = ydiffs * X[1,:]\n",
    "g2 = ydiffs * X[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for w0 (1st row), w1 and w2 (last row) for ALL examples.\n",
      "Each column represent an example.  Five in total.\n",
      "[ -99.49519886 -111.03223123 -109.21172544 -104.2711384   -90.53712043]\n",
      "[-1540.99507572 -1904.41302337 -1750.40553776 -1610.86728187 -1288.93606021]\n",
      "[-356.54865175 -305.37736894 -498.77999791 -506.19999069 -229.39988431]\n"
     ]
    }
   ],
   "source": [
    "print \"Gradient for w0 (1st row), w1 and w2 (last row) for ALL examples.\"\n",
    "print \"Each column represent an example.  Five in total.\"\n",
    "print g0\n",
    "print g1\n",
    "print g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for ALL examples.\n",
      "Each row represent a component in the weights (e.g. w0 = 1st row)\n",
      "Each column represent an example.\n",
      "\n",
      "[[  -99.49519886  -111.03223123  -109.21172544  -104.2711384    -90.53712043]\n",
      " [-1540.99507572 -1904.41302337 -1750.40553776 -1610.86728187\n",
      "  -1288.93606021]\n",
      " [ -356.54865175  -305.37736894  -498.77999791  -506.19999069\n",
      "   -229.39988431]]\n"
     ]
    }
   ],
   "source": [
    "# This will give the same result\n",
    "g = ydiffs * X\n",
    "print \"Gradient for ALL examples.\"\n",
    "print \"Each row represent a component in the weights (e.g. w0 = 1st row)\"\n",
    "print \"Each column represent an example.\"\n",
    "print\n",
    "print g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (Again)\n",
    "# Algorithm:\n",
    "# Starts with some “initial guess” for θ\n",
    "# Repeat:\n",
    "#     Changes θ to make J(θ) smaller:\n",
    "#        θj := θj - alpha * gradient_of_J_wrt_θj \n",
    "#        where j is the j-th component of θ. (θ0 = w0, θ1 = w1, θ2 = w2)  \n",
    "# until we converge to a value of θ that minimizes J(θ)\n",
    "\n",
    "# If we only have ONE single training example:\n",
    "#\n",
    "# Gradient_of_J_wrt_θj = d J(θ) / d θj = \n",
    "# d 1/2 sum_over_all_X_y_pairs((h_θ(xi) - yi)^2)/ d θj = \n",
    "# (h_θ(x) - y)x_j\n",
    "\n",
    "# If we have many training example:\n",
    "#\n",
    "# Gradient_of_J_wrt_θj = alpha * sum_over_all_examples((h_θ(x) - y)x_j) / number_of_examples\n",
    "#\n",
    "# Note: we have to average it out over the examples, otherwise gradient will increase as sample size increases.\n",
    "g_sum_over_examples = np.sum(g, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each column represents the sum of gradient from all examples.  1st column means w0.\n",
      "[-0.3942039   0.79810913 -4.0575618 ]\n"
     ]
    }
   ],
   "source": [
    "print \"Each column represents the sum of gradient from all examples.  1st column means w0.\"\n",
    "print g_sum_over_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wdelta = -1 * alpha * g_sum_over_examples / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.88407806e-05  -1.59621825e-04   8.11512360e-04]\n"
     ]
    }
   ],
   "source": [
    "print wdelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = w + wdelta\n",
    "print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True formula: y = 2 + x1*5 + x2*4 + noise\n",
      "After 1000  iterations, w =  [ 0.54728746  5.18380913  3.52768918]\n"
     ]
    }
   ],
   "source": [
    "# Let's put everything under a loop\n",
    "size = 1000\n",
    "X, y = genData(size, noise_mean = 0, noise_sd = 0.5)\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.001\n",
    "stopping_critera = 0.0001\n",
    "w = [0,0,0] # Initialize the weight\n",
    "iterations = 1000\n",
    "\n",
    "for i in range(iterations):\n",
    "    yhats = np.dot(w, X)\n",
    "    ydiffs = yhats - y # (h_θ(x) - y) for all training examples\n",
    "    g = ydiffs * X\n",
    "    g_sum_over_examples = np.sum(g, axis=1)\n",
    "    wdelta = -1 * alpha * g_sum_over_examples / size\n",
    "    \n",
    "    # Convergence check\n",
    "    if np.linalg.norm(wdelta) <= stopping_critera:\n",
    "        break\n",
    "        \n",
    "    #print \"Iteration\", i, \"- wdelta =\", wdelta\n",
    "    w = w + wdelta\n",
    "\n",
    "print \"True formula: y = 2 + x1*5 + x2*4 + noise\"\n",
    "print \"After\", i+1, \" iterations, w = \", w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Batch Gradient Descent, in each loop we update w by batching up the changes from all the examples.\n",
    "- In Stochastic Gradient Descent, within each loop, we loop through all examples, and update w for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True formula: y = 2 + x1*5 + x2*4 + noise\n",
      "After 500  iterations, w =  [ 2.05590869  4.9859899   4.00956634]\n"
     ]
    }
   ],
   "source": [
    "# Let's put everything under a loop\n",
    "size = 1000\n",
    "X, y = genData(size, noise_mean = 0, noise_sd = 0.2)\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.005\n",
    "stopping_critera = 0.0001\n",
    "w = [0,0,0] # Initialize the weight\n",
    "iterations = 500\n",
    "\n",
    "for i in range(iterations):\n",
    "    for m in range(size):\n",
    "        x = X[:,m] # a single example\n",
    "        y_single = y[m]\n",
    "        \n",
    "        # (h_θ(x) - y) for one training example\n",
    "        yhat = np.dot(w, x) \n",
    "        ydiff = yhat - y_single\n",
    "        \n",
    "        # Gradient for one example        \n",
    "        g = ydiff * x\n",
    "        \n",
    "        wdelta = -1 * alpha * g\n",
    "    \n",
    "        # Convergence check\n",
    "        if np.linalg.norm(wdelta) <= stopping_critera:\n",
    "            break\n",
    "        \n",
    "        w = w + wdelta\n",
    "\n",
    "print \"True formula: y = 2 + x1*5 + x2*4 + noise\"\n",
    "print \"After\", i+1, \" iterations, w = \", w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Try Batch Gradient Descent with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True formula: y = 2 + x1*5 + x2*4 + noise\n",
      "After 500  iterations, w =  [ 0.63756532  5.08123991  4.03859815]\n"
     ]
    }
   ],
   "source": [
    "# Let's put everything under a loop\n",
    "size = 1000\n",
    "X, y = genData(size, noise_mean = 0, noise_sd = 0.5)\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.005\n",
    "stopping_critera = 0.0001\n",
    "w = [0,0,0] # Initialize the weight\n",
    "iterations = 500\n",
    "\n",
    "# Convert X, y into an array of Point object.  Note that each column in X is an x, so we\n",
    "# need to do the transpose (X.T), otherwise we'll be getting each row of X instead of each column.\n",
    "points = [Point(one_x, one_y) for one_x, one_y in zip(X.T, y)]\n",
    "data = sc.parallelize(points).cache()\n",
    "\n",
    "for i in range(iterations):\n",
    "    # If we don't use broadcast, the driver has to send out w for each task.\n",
    "    wbroadcast = sc.broadcast(w)\n",
    "\n",
    "    # Run this for each single example to get the partial gradient\n",
    "    # (h_θ(x) - y)x_j\n",
    "    partial_gradients = data.map(lambda point: (np.dot(wbroadcast.value, point.x)- point.y)*point.x)\n",
    "    \n",
    "    # Add up the partial gradient from all the example, and average them out.\n",
    "    g = partial_gradients.reduce(lambda a, b: a + b)/size\n",
    "    \n",
    "    wdelta = -1 * alpha * g\n",
    "    \n",
    "    # Convergence check\n",
    "    if np.linalg.norm(wdelta) <= stopping_critera:\n",
    "        break\n",
    "        \n",
    "    #print \"Iteration\", i, \"- wdelta =\", wdelta\n",
    "    w = w + wdelta\n",
    "\n",
    "print \"True formula: y = 2 + x1*5 + x2*4 + noise\"\n",
    "print \"After\", i+1, \" iterations, w = \", w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
