{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRJob Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary Sort\n",
    "Secondary using the 3rd key in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.data\n",
    "4,10,3,Apple\n",
    "2,2,4,Orange\n",
    "6,-1,6,Lemon\n",
    "0,9,18,Apple\n",
    "6,8,7,Lemon\n",
    "6,199,20,Lemon\n",
    "6,-9,2,Lemon\n",
    "6,-1,10,Lemon\n",
    "6,-9223372036854775808,43,Orange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- commented out\n",
    "jobconf={\n",
    "    \"stream.num.map.output.key.fields\":\"3\",\n",
    "    \"mapreduce.job.output.key.comparator.class\":\n",
    "        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "    \"mapreduce.partition.keycomparator.options\":\"-k1,1n -k3,3nr\",\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "\n",
    "import sys\n",
    "def toStringKey(n):\n",
    "    n = int(n)\n",
    "    digits = len(str(sys.maxint))\n",
    "    minInt = -sys.maxint - 1\n",
    "\n",
    "    if n < 0:\n",
    "        key = \"-\" + str(abs(minInt-n)).zfill(digits)\n",
    "    else:\n",
    "        key = str(n).zfill(digits)\n",
    "        \n",
    "    return key\n",
    "    \n",
    "class test(MRJob):\n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def mapper1(self, line_no, line):\n",
    "        cell = line.strip().split(',')\n",
    "        \n",
    "        yield cell[0], [toStringKey(cell[1])] + cell[1:]\n",
    "\n",
    "    def reducer1(self, key, value):\n",
    "        yield key, [v for v in value]\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1,\n",
    "                   reducer=self.reducer1,\n",
    "        )]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.sim:ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "\"0\"\t[[\"0000000000000000009\", \"9\", \"18\", \"Apple\"]]\n",
      "\n",
      "\"2\"\t[[\"0000000000000000002\", \"2\", \"4\", \"Orange\"]]\n",
      "\n",
      "\"4\"\t[[\"0000000000000000010\", \"10\", \"3\", \"Apple\"]]\n",
      "\n",
      "\"6\"\t[[\"-0000000000000000000\", \"-9223372036854775808\", \"43\", \"Orange\"], [\"-9223372036854775799\", \"-9\", \"2\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"10\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"6\", \"Lemon\"], [\"0000000000000000008\", \"8\", \"7\", \"Lemon\"], [\"0000000000000000199\", \"199\", \"20\", \"Lemon\"]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from test import test\n",
    "mr_job = test(args=['test.data', '-r', 'inline', '--no-strict-protocols'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    print \"Output:\"\n",
    "    for line in runner.stream_output():\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the input to mapper has terminating newline or not.\n",
    "Answer: NO\n",
    "\n",
    "### How about passing a list inside a tuple as values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test2.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class test2(MRJob):\n",
    "    def mapper1(self, line_no, line):\n",
    "        fields = line.split(',')\n",
    "        v = [\"a\",\"b\",\"c\"]\n",
    "        yield fields[0], (v, len(fields))\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        items = []\n",
    "        for v in values:\n",
    "            yield key, (\"#\".join(v[0]), v[1])\n",
    "        \n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1,\n",
    "                  reducer=self.reducer1)\n",
    "            ]\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test2.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/0/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/0/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python test2.py --step-num=0 --mapper /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/input_part-00000\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/1/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/1/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python test2.py --step-num=0 --mapper /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/input_part-00001\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/reducer/0/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/reducer/0/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python test2.py --step-num=0 --reducer /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/input_part-00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "('0', ['a#b#c', 4])\n",
      "('2', ['a#b#c', 4])\n",
      "('4', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n"
     ]
    }
   ],
   "source": [
    "from test2 import test2\n",
    "mr_job = test2(args=['test.data', '-r', 'local'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    print \"Output:\"\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
