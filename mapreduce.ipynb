{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MRJob Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary Sort\n",
    "Secondary using the 3rd key in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.data\n",
    "4,10,3,Apple\n",
    "2,2,4,Orange\n",
    "6,-1,6,Lemon\n",
    "0,9,18,Apple\n",
    "6,8,7,Lemon\n",
    "6,199,20,Lemon\n",
    "6,-9,2,Lemon\n",
    "6,-1,10,Lemon\n",
    "6,-9223372036854775808,43,Orange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- commented out\n",
    "jobconf={\n",
    "    \"stream.num.map.output.key.fields\":\"3\",\n",
    "    \"mapreduce.job.output.key.comparator.class\":\n",
    "        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "    \"mapreduce.partition.keycomparator.options\":\"-k1,1n -k3,3nr\",\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "\n",
    "import sys\n",
    "def toStringKey(n):\n",
    "    n = int(n)\n",
    "    digits = len(str(sys.maxint))\n",
    "    minInt = -sys.maxint - 1\n",
    "\n",
    "    if n < 0:\n",
    "        key = \"-\" + str(abs(minInt-n)).zfill(digits)\n",
    "    else:\n",
    "        key = str(n).zfill(digits)\n",
    "        \n",
    "    return key\n",
    "    \n",
    "class test(MRJob):\n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def mapper(self, line_no, line):\n",
    "        cell = line.strip().split(',')\n",
    "        \n",
    "        yield cell[0], [toStringKey(cell[1])] + cell[1:]\n",
    "\n",
    "    def reducer(self, key, value):\n",
    "        self.increment_counter(\"patng\", \"hello\", amount=1)\n",
    "        yield key, [v for v in value]\n",
    "        \n",
    "    def reducer_final(self):\n",
    "        with open(\"result.txt\", \"w\") as f:\n",
    "            f.write(\"hello\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.sim:ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "\"0\"\t[[\"0000000000000000009\", \"9\", \"18\", \"Apple\"]]\n",
      "\n",
      "\"2\"\t[[\"0000000000000000002\", \"2\", \"4\", \"Orange\"]]\n",
      "\n",
      "\"4\"\t[[\"0000000000000000010\", \"10\", \"3\", \"Apple\"]]\n",
      "\n",
      "\"6\"\t[[\"-0000000000000000000\", \"-9223372036854775808\", \"43\", \"Orange\"], [\"-9223372036854775799\", \"-9\", \"2\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"10\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"6\", \"Lemon\"], [\"0000000000000000008\", \"8\", \"7\", \"Lemon\"], [\"0000000000000000199\", \"199\", \"20\", \"Lemon\"]]\n",
      "\n",
      "Counters: [{'patng': {'hello': 4}}]\n"
     ]
    }
   ],
   "source": [
    "from test import test\n",
    "mr_job = test(args=['test.data', '-r', 'inline', '--strict-protocols'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    print \"Output:\"\n",
    "    for line in runner.stream_output():\n",
    "        print line\n",
    "        \n",
    "    print \"Counters:\", runner.counters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing using Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/10 11:35:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: namenode running as process 1489. Stop it first.\n",
      "localhost: datanode running as process 1581. Stop it first.\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: secondarynamenode running as process 1696. Stop it first.\n",
      "16/03/10 11:35:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "starting yarn daemons\n",
      "resourcemanager running as process 1826. Stop it first.\n",
      "localhost: nodemanager running as process 1927. Stop it first.\n",
      "/usr/local/Cellar/hadoop/2.7.1/libexec/sbin/start-historyserver.sh: line 1: ./mr-jobhistory-daemon.sh: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/libexec/sbin/start-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/libexec/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/libexec/sbin/start-historyserver.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test1.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile test1.data\n",
    "4,10,3,Chair\n",
    "2,2,4,Desk\n",
    "6,-1,6,Lamp\n",
    "0,9,18,Chair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test2.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile test2.data\n",
    "12,1,1,Pencil\n",
    "6,8,7,Ball\n",
    "6,199,20,Ball\n",
    "6,-9,2,Ball\n",
    "6,-1,10,Ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting badData.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile badData.data\n",
    "12,1,1,BAD\n",
    "6,8,7,BAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/09 09:48:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/03/09 09:48:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -f learn_Testinput\n",
    "!hdfs dfs -mkdir learn_Testinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/09 09:50:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `learn_Testinput/test1.data': File exists\n",
      "put: `learn_Testinput/test2.data': File exists\n",
      "16/03/09 09:50:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put test?.data learn_Testinput/\n",
    "!hdfs dfs -put badData.data learn_Testinput/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/10 10:03:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   1 patrickng supergroup         20 2016-03-09 09:50 hdfs://127.0.0.1/user/patrickng/learn_Testinput/badData.data\n",
      "-rw-r--r--   1 patrickng supergroup         51 2016-03-09 09:49 hdfs://127.0.0.1/user/patrickng/learn_Testinput/test1.data\n",
      "-rw-r--r--   1 patrickng supergroup         67 2016-03-09 09:49 hdfs://127.0.0.1/user/patrickng/learn_Testinput/test2.data\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls hdfs://127.0.0.1/user/patrickng/learn_Testinput/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it on Hadoop with output streamed out to the driver\n",
    "**Note:** \n",
    "+ You can use regular expression in the input.  Or you can specify the whole folder.\n",
    "+ Custom counters **don't work** in Hadoop or EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "\"0\"\t[[\"0000000000000000009\", \"9\", \"18\", \"Apple\"]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:mrjob.fs.hadoop:STDERR: 16/03/10 10:03:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"12\"\t[[\"0000000000000000001\", \"1\", \"1\", \"Banana\"]]\n",
      "\n",
      "\"2\"\t[[\"0000000000000000002\", \"2\", \"4\", \"Orange\"]]\n",
      "\n",
      "\"4\"\t[[\"0000000000000000010\", \"10\", \"3\", \"Apple\"]]\n",
      "\n",
      "\"6\"\t[[\"-9223372036854775799\", \"-9\", \"2\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"10\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"6\", \"Lemon\"], [\"0000000000000000008\", \"8\", \"7\", \"Lemon\"], [\"0000000000000000199\", \"199\", \"20\", \"Lemon\"]]\n",
      "\n",
      "Counters: [{}]\n"
     ]
    }
   ],
   "source": [
    "from test import test\n",
    "mr_job = test(args=['hdfs://127.0.0.1/user/patrickng/learn_Testinput/test?.data', \n",
    "                    '-r', 'hadoop', \n",
    "                    '--strict-protocols'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    print \"Output:\"\n",
    "    for line in runner.stream_output():\n",
    "        print line\n",
    "        \n",
    "    print \"Counters:\", runner.counters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0\"\t[[\"0000000000000000009\", \"9\", \"18\", \"Apple\"]]\r\n",
      "\"12\"\t[[\"0000000000000000001\", \"1\", \"1\", \"Banana\"]]\r\n",
      "\"2\"\t[[\"0000000000000000002\", \"2\", \"4\", \"Orange\"]]\r\n",
      "\"4\"\t[[\"0000000000000000010\", \"10\", \"3\", \"Apple\"]]\r\n",
      "\"6\"\t[[\"-9223372036854775799\", \"-9\", \"2\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"10\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"6\", \"Lemon\"], [\"0000000000000000008\", \"8\", \"7\", \"Lemon\"], [\"0000000000000000199\", \"199\", \"20\", \"Lemon\"]]\r\n"
     ]
    }
   ],
   "source": [
    "## Run the program with input and output in Hadoop\n",
    "!python test.py \\\n",
    "--strict-protocols \\\n",
    "hdfs://127.0.0.1/user/patrickng/learn_Testinput/test?.data \\\n",
    "-r hadoop \\\n",
    "-q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it on Hadoop with output saved to an HDFS folder, plus streaming down the output at the same time.\n",
    "**Note:**\n",
    "+ The output folder **cannot exists**, otherwise the job will fail..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/09 12:54:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted learn_Testoutput\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r learn_Testoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test.patrickng.20160309.045408.971826\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test.patrickng.20160309.045408.971826/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/test.patrickng.20160309.045408.971826/files/\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar5365450498642417801/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob278763983399947279.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs://127.0.0.1/user/patrickng/learn_Testoutput\n",
      "\"0\"\t[[\"0000000000000000009\", \"9\", \"18\", \"Apple\"]]\n",
      "\"12\"\t[[\"0000000000000000001\", \"1\", \"1\", \"Banana\"]]\n",
      "\"2\"\t[[\"0000000000000000002\", \"2\", \"4\", \"Orange\"]]\n",
      "\"4\"\t[[\"0000000000000000010\", \"10\", \"3\", \"Apple\"]]\n",
      "\"6\"\t[[\"-9223372036854775799\", \"-9\", \"2\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"10\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"6\", \"Lemon\"], [\"0000000000000000008\", \"8\", \"7\", \"Lemon\"], [\"0000000000000000199\", \"199\", \"20\", \"Lemon\"]]\n",
      "STDERR: 16/03/09 12:54:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test.patrickng.20160309.045408.971826\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/test.patrickng.20160309.045408.971826 from HDFS\n"
     ]
    }
   ],
   "source": [
    "## Run the program with input and output in Hadoop\n",
    "!python test.py \\\n",
    "--strict-protocols \\\n",
    "hdfs://127.0.0.1/user/patrickng/learn_Testinput/test?.data \\\n",
    "-r hadoop \\\n",
    "--output-dir hdfs://127.0.0.1/user/patrickng/learn_Testoutput \n",
    "# Note:\n",
    "# We're not using --no-output here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/09 12:55:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   1 patrickng supergroup          0 2016-03-09 12:54 learn_Testoutput/_SUCCESS\n",
      "-rw-r--r--   1 patrickng supergroup        436 2016-03-09 12:54 learn_Testoutput/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls learn_Testoutput/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/09 11:27:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"0\"\t[[\"0000000000000000009\", \"9\", \"18\", \"Apple\"]]\n",
      "\"12\"\t[[\"0000000000000000001\", \"1\", \"1\", \"Banana\"]]\n",
      "\"2\"\t[[\"0000000000000000002\", \"2\", \"4\", \"Orange\"]]\n",
      "\"4\"\t[[\"0000000000000000010\", \"10\", \"3\", \"Apple\"]]\n",
      "\"6\"\t[[\"-9223372036854775799\", \"-9\", \"2\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"10\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"6\", \"Lemon\"], [\"0000000000000000008\", \"8\", \"7\", \"Lemon\"], [\"0000000000000000199\", \"199\", \"20\", \"Lemon\"]]\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat learn_Testoutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing using EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key= s3.ObjectSummary(bucket_name='patng323-learn-mrjob', key='input/badData.data')\n",
      "key= s3.ObjectSummary(bucket_name='patng323-learn-mrjob', key='input/test1.data')\n",
      "key= s3.ObjectSummary(bucket_name='patng323-learn-mrjob', key='input/test2.data')\n",
      "key= s3.ObjectSummary(bucket_name='patng323-learn-mrjob', key='output/_SUCCESS')\n",
      "key= s3.ObjectSummary(bucket_name='patng323-learn-mrjob', key='output/part-00000')\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "name = \"patng323-learn-mrjob\"\n",
    "\n",
    "bucket = s3.Bucket(name)\n",
    "exists = True\n",
    "\n",
    "# Check if the bucket if it exists\n",
    "try:\n",
    "    s3.meta.client.head_bucket(Bucket=name)\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    error_code = int(e.response['Error']['Code'])\n",
    "    if error_code == 404:\n",
    "        exists = False\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "if not exists:\n",
    "    s3.create_bucket(Bucket=name)\n",
    "else:\n",
    "    # Clear all items (including \"folders\") if it exists\n",
    "    for key in bucket.objects.all():\n",
    "        key.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'ETag': '\"234e16b14d93a16aa97b4621c51ebf16\"',\n",
       " 'ResponseMetadata': {'HTTPStatusCode': 200,\n",
       "  'HostId': 'C9DmLd9jFF8LVEqyXsDCoqscJdbraD0cs+c/wKsGNCEu9FsNzLc7tYkj83fz20QV',\n",
       "  'RequestId': '0452644BA8E9FE47'}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload files to S3\n",
    "s3.Object(name, 'input/test1.data').put(Body=open('test1.data', 'rb'))\n",
    "s3.Object(name, 'input/test2.data').put(Body=open('test2.data', 'rb'))\n",
    "s3.Object(name, 'input/badData.data').put(Body=open('badData.data', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-09 10:22:12         20 badData.data\r\n",
      "2016-03-09 10:22:11         48 test1.data\r\n",
      "2016-03-09 10:22:11         63 test2.data\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls patng323-learn-mrjob/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start an EMR job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-0c26425c25d7acc1\n",
      "using s3://mrjob-0c26425c25d7acc1/tmp/ as our scratch dir on S3\n",
      "Creating persistent job flow to run several jobs in...\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/no_script.patrickng.20160310.014237.369516\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/no_script.patrickng.20160310.014237.369516/b.py\n",
      "Copying non-input files into s3://mrjob-0c26425c25d7acc1/tmp/no_script.patrickng.20160310.014237.369516/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-1EI9DBDPZO66S\n",
      "j-1EI9DBDPZO66S\n"
     ]
    }
   ],
   "source": [
    "# Don't forget to terminate it when it's not needed\n",
    "!mrjob create-job-flow --num-ec2-instances=1 \\\n",
    "--ec2-instance-type=m1.medium \\\n",
    "--max-hours-idle=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://patng323-learn-mrjob/output/_SUCCESS\n",
      "delete: s3://patng323-learn-mrjob/output/part-00000\n",
      "Wait 5.0s sec for S3 eventual consistency\n",
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using configs in /Users/patrickng/.mrjob.conf\n",
      "using existing scratch bucket mrjob-0c26425c25d7acc1\n",
      "using s3://mrjob-0c26425c25d7acc1/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test.patrickng.20160310.030437.424045\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Copying non-input files into s3://mrjob-0c26425c25d7acc1/tmp/test.patrickng.20160310.030437.424045/files/\n",
      "Adding our job to existing job flow j-1EI9DBDPZO66S\n",
      "Job launched 32.6s ago, status RUNNING: Running step (test.patrickng.20160310.030437.424045: Step 1 of 1)\n",
      "Job launched 65.6s ago, status RUNNING: Running step (test.patrickng.20160310.030437.424045: Step 1 of 1)\n",
      "Job launched 98.2s ago, status RUNNING: Running step (test.patrickng.20160310.030437.424045: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 66.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters may not have been uploaded to S3 yet. Try again in 5 minutes with: mrjob fetch-logs --counters j-1EI9DBDPZO66S\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test.patrickng.20160310.030437.424045\n",
      "Removing all files in s3://mrjob-0c26425c25d7acc1/tmp/test.patrickng.20160310.030437.424045/\n"
     ]
    }
   ],
   "source": [
    "# Note: the output directory MUST not exist\n",
    "!aws s3 rm --recursive s3://patng323-learn-mrjob/output\n",
    "!echo \"Wait 5.0s sec for S3 eventual consistency\"\n",
    "!sleep 5\n",
    "\n",
    "!python test.py \\\n",
    "s3://patng323-learn-mrjob/input/test?.data \\\n",
    "-r emr \\\n",
    "--emr-job-flow-id j-1EI9DBDPZO66S \\\n",
    "--no-output \\\n",
    "--output-dir s3://patng323-learn-mrjob/output/ \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-10 11:06:07          0 _SUCCESS\r\n",
      "2016-03-10 11:06:02        429 part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://patng323-learn-mrjob/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://patng323-learn-mrjob/output/part-00000 to ./part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://patng323-learn-mrjob/output/part-00000 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0\"\t[[\"0000000000000000009\", \"9\", \"18\", \"Chair\"]]\r\n",
      "\"12\"\t[[\"0000000000000000001\", \"1\", \"1\", \"Pencil\"]]\r\n",
      "\"2\"\t[[\"0000000000000000002\", \"2\", \"4\", \"Desk\"]]\r\n",
      "\"4\"\t[[\"0000000000000000010\", \"10\", \"3\", \"Chair\"]]\r\n",
      "\"6\"\t[[\"-9223372036854775799\", \"-9\", \"2\", \"Ball\"], [\"-9223372036854775807\", \"-1\", \"10\", \"Ball\"], [\"-9223372036854775807\", \"-1\", \"6\", \"Lamp\"], [\"0000000000000000008\", \"8\", \"7\", \"Ball\"], [\"0000000000000000199\", \"199\", \"20\", \"Ball\"]]\r\n"
     ]
    }
   ],
   "source": [
    "!cat part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this if you want to clear the output \"folder\" in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "name = \"patng323-learn-mrjob\"\n",
    "\n",
    "bucket = s3.Bucket(name)\n",
    "# Clear all items (including \"folders\") if it exists\n",
    "for object in bucket.objects.all():\n",
    "    if object.key.startswith('output/'):\n",
    "        object.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls patng323-learn-mrjob/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget to terminate the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-03ccebffb1b98a81\n",
      "using s3://mrjob-03ccebffb1b98a81/tmp/ as our scratch dir on S3\n",
      "Terminated job flow j-2X9F8BNAZZYIN\n"
     ]
    }
   ],
   "source": [
    "!mrjob terminate-job-flow j-2X9F8BNAZZYIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the input to mapper has terminating newline or not.\n",
    "Answer: NO\n",
    "\n",
    "### How about passing a list inside a tuple as values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test2.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class test2(MRJob):\n",
    "    def mapper1(self, line_no, line):\n",
    "        fields = line.split(',')\n",
    "        v = [\"a\",\"b\",\"c\"]\n",
    "        yield fields[0], (v, len(fields))\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        items = []\n",
    "        for v in values:\n",
    "            yield key, (\"#\".join(v[0]), v[1])\n",
    "        \n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1,\n",
    "                  reducer=self.reducer1)\n",
    "            ]\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test2.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/0/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/0/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python test2.py --step-num=0 --mapper /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/input_part-00000\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/1/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/1/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python test2.py --step-num=0 --mapper /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/input_part-00001\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/reducer/0/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/reducer/0/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python test2.py --step-num=0 --reducer /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/input_part-00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "('0', ['a#b#c', 4])\n",
      "('2', ['a#b#c', 4])\n",
      "('4', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n"
     ]
    }
   ],
   "source": [
    "from test2 import test2\n",
    "mr_job = test2(args=['test.data', '-r', 'local', '--strict-protocols'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    print \"Output:\"\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
