{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRJob Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary Sort\n",
    "Secondary using the 3rd key in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.data\n",
    "4,10,3,Apple\n",
    "2,2,4,Orange\n",
    "6,-1,6,Lemon\n",
    "0,9,18,Apple\n",
    "6,8,7,Lemon\n",
    "6,199,20,Lemon\n",
    "6,-9,2,Lemon\n",
    "6,-1,10,Lemon\n",
    "6,-9223372036854775808,43,Orange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- commented out\n",
    "jobconf={\n",
    "    \"stream.num.map.output.key.fields\":\"3\",\n",
    "    \"mapreduce.job.output.key.comparator.class\":\n",
    "        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "    \"mapreduce.partition.keycomparator.options\":\"-k1,1n -k3,3nr\",\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "\n",
    "import sys\n",
    "def toStringKey(n):\n",
    "    n = int(n)\n",
    "    digits = len(str(sys.maxint))\n",
    "    minInt = -sys.maxint - 1\n",
    "\n",
    "    if n < 0:\n",
    "        key = \"-\" + str(abs(minInt-n)).zfill(digits)\n",
    "    else:\n",
    "        key = str(n).zfill(digits)\n",
    "        \n",
    "    return key\n",
    "    \n",
    "class test(MRJob):\n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def mapper1(self, line_no, line):\n",
    "        cell = line.strip().split(',')\n",
    "        \n",
    "        yield cell[0], [toStringKey(cell[1])] + cell[1:]\n",
    "\n",
    "    def reducer1(self, key, value):\n",
    "        yield key, [v for v in value]\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1,\n",
    "                   reducer=self.reducer1,\n",
    "        )]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.sim:ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "\"0\"\t[[\"0000000000000000009\", \"9\", \"18\", \"Apple\"]]\n",
      "\n",
      "\"2\"\t[[\"0000000000000000002\", \"2\", \"4\", \"Orange\"]]\n",
      "\n",
      "\"4\"\t[[\"0000000000000000010\", \"10\", \"3\", \"Apple\"]]\n",
      "\n",
      "\"6\"\t[[\"-0000000000000000000\", \"-9223372036854775808\", \"43\", \"Orange\"], [\"-9223372036854775799\", \"-9\", \"2\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"10\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"6\", \"Lemon\"], [\"0000000000000000008\", \"8\", \"7\", \"Lemon\"], [\"0000000000000000199\", \"199\", \"20\", \"Lemon\"]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from test import test\n",
    "mr_job = test(args=['test.data', '-r', 'inline', '--no-strict-protocols'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    print \"Output:\"\n",
    "    for line in runner.stream_output():\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing using Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/09 09:47:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-namenode-Patricks-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-datanode-Patricks-MacBook-Pro.local.out\n",
      "localhost: 16/03/09 09:47:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-secondarynamenode-Patricks-MacBook-Pro.local.out\n",
      "0.0.0.0: 16/03/09 09:47:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/03/09 09:47:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-patrickng-resourcemanager-Patricks-MacBook-Pro.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-patrickng-nodemanager-Patricks-MacBook-Pro.local.out\n",
      "/usr/local/Cellar/hadoop/2.7.1/libexec/sbin/start-historyserver.sh: line 1: ./mr-jobhistory-daemon.sh: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/libexec/sbin/start-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/libexec/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/libexec/sbin/start-historyserver.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test1.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile test1.data\n",
    "4,10,3,Chair\n",
    "2,2,4,Desk\n",
    "6,-1,6,Lamp\n",
    "0,9,18,Chair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test2.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile test2.data\n",
    "12,1,1,Pencil\n",
    "6,8,7,Ball\n",
    "6,199,20,Ball\n",
    "6,-9,2,Ball\n",
    "6,-1,10,Ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting badData.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile badData.data\n",
    "12,1,1,BAD\n",
    "6,8,7,BAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/09 09:48:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/03/09 09:48:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -f learn_Testinput\n",
    "!hdfs dfs -mkdir learn_Testinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/09 09:50:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `learn_Testinput/test1.data': File exists\n",
      "put: `learn_Testinput/test2.data': File exists\n",
      "16/03/09 09:50:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put test?.data learn_Testinput/\n",
    "!hdfs dfs -put badData.data learn_Testinput/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/09 10:05:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   1 patrickng supergroup         20 2016-03-09 09:50 hdfs://127.0.0.1/user/patrickng/learn_Testinput/badData.data\n",
      "-rw-r--r--   1 patrickng supergroup         51 2016-03-09 09:49 hdfs://127.0.0.1/user/patrickng/learn_Testinput/test1.data\n",
      "-rw-r--r--   1 patrickng supergroup         67 2016-03-09 09:49 hdfs://127.0.0.1/user/patrickng/learn_Testinput/test2.data\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls hdfs://127.0.0.1/user/patrickng/learn_Testinput/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it on Hadoop with output streamed out to the driver\n",
    "**Note:** You can use regular expression in the input.  Or you can specify the whole folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0\"\t[[\"0000000000000000009\", \"9\", \"18\", \"Apple\"]]\r\n",
      "\"12\"\t[[\"0000000000000000001\", \"1\", \"1\", \"Banana\"]]\r\n",
      "\"2\"\t[[\"0000000000000000002\", \"2\", \"4\", \"Orange\"]]\r\n",
      "\"4\"\t[[\"0000000000000000010\", \"10\", \"3\", \"Apple\"]]\r\n",
      "\"6\"\t[[\"-9223372036854775799\", \"-9\", \"2\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"10\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"6\", \"Lemon\"], [\"0000000000000000008\", \"8\", \"7\", \"Lemon\"], [\"0000000000000000199\", \"199\", \"20\", \"Lemon\"]]\r\n"
     ]
    }
   ],
   "source": [
    "## Run the program with input and output in Hadoop\n",
    "!python test.py \\\n",
    "--strict-protocols \\\n",
    "hdfs://127.0.0.1/user/patrickng/learn_Testinput/test?.data \\\n",
    "-r hadoop \\\n",
    "-q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it on Hadoop with output saved to an HDFS folder\n",
    "**Note:**\n",
    "+ The output folder **cannot exists**, otherwise the job will fail..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Run the program with input and output in Hadoop\n",
    "!python test.py \\\n",
    "--strict-protocols \\\n",
    "hdfs://127.0.0.1/user/patrickng/learn_Testinput/test?.data \\\n",
    "-r hadoop \\\n",
    "--no-output \\\n",
    "--output-dir hdfs://127.0.0.1/user/patrickng/learn_Testoutput \\\n",
    "-q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/09 10:14:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   1 patrickng supergroup          0 2016-03-09 10:14 learn_Testoutput/_SUCCESS\n",
      "-rw-r--r--   1 patrickng supergroup        436 2016-03-09 10:14 learn_Testoutput/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls learn_Testoutput/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/09 10:10:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"0\"\t[[\"0000000000000000009\", \"9\", \"18\", \"Apple\"]]\n",
      "\"12\"\t[[\"0000000000000000001\", \"1\", \"1\", \"Banana\"]]\n",
      "\"2\"\t[[\"0000000000000000002\", \"2\", \"4\", \"Orange\"]]\n",
      "\"4\"\t[[\"0000000000000000010\", \"10\", \"3\", \"Apple\"]]\n",
      "\"6\"\t[[\"-9223372036854775799\", \"-9\", \"2\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"10\", \"Lemon\"], [\"-9223372036854775807\", \"-1\", \"6\", \"Lemon\"], [\"0000000000000000008\", \"8\", \"7\", \"Lemon\"], [\"0000000000000000199\", \"199\", \"20\", \"Lemon\"]]\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat learn_Testoutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing using EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key= s3.ObjectSummary(bucket_name='patng323-learn-mrjob', key='input/badData.data')\n",
      "key= s3.ObjectSummary(bucket_name='patng323-learn-mrjob', key='input/test1.data')\n",
      "key= s3.ObjectSummary(bucket_name='patng323-learn-mrjob', key='input/test2.data')\n",
      "key= s3.ObjectSummary(bucket_name='patng323-learn-mrjob', key='output/_SUCCESS')\n",
      "key= s3.ObjectSummary(bucket_name='patng323-learn-mrjob', key='output/part-00000')\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "name = \"patng323-learn-mrjob\"\n",
    "\n",
    "bucket = s3.Bucket(name)\n",
    "exists = True\n",
    "\n",
    "# Check if the bucket if it exists\n",
    "try:\n",
    "    s3.meta.client.head_bucket(Bucket=name)\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    error_code = int(e.response['Error']['Code'])\n",
    "    if error_code == 404:\n",
    "        exists = False\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "if not exists:\n",
    "    s3.create_bucket(Bucket=name)\n",
    "else:\n",
    "    # Clear all items (including \"folders\") if it exists\n",
    "    for key in bucket.objects.all():\n",
    "        key.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'ETag': '\"234e16b14d93a16aa97b4621c51ebf16\"',\n",
       " 'ResponseMetadata': {'HTTPStatusCode': 200,\n",
       "  'HostId': 'C9DmLd9jFF8LVEqyXsDCoqscJdbraD0cs+c/wKsGNCEu9FsNzLc7tYkj83fz20QV',\n",
       "  'RequestId': '0452644BA8E9FE47'}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload files to S3\n",
    "s3.Object(name, 'input/test1.data').put(Body=open('test1.data', 'rb'))\n",
    "s3.Object(name, 'input/test2.data').put(Body=open('test2.data', 'rb'))\n",
    "s3.Object(name, 'input/badData.data').put(Body=open('badData.data', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-09 10:22:12         20 badData.data\r\n",
      "2016-03-09 10:22:11         48 test1.data\r\n",
      "2016-03-09 10:22:11         63 test2.data\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls patng323-learn-mrjob/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start an EMR job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-03ccebffb1b98a81\n",
      "using s3://mrjob-03ccebffb1b98a81/tmp/ as our scratch dir on S3\n",
      "Creating persistent job flow to run several jobs in...\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/no_script.patrickng.20160309.021820.035465\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/no_script.patrickng.20160309.021820.035465/b.py\n",
      "Copying non-input files into s3://mrjob-03ccebffb1b98a81/tmp/no_script.patrickng.20160309.021820.035465/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-2X9F8BNAZZYIN\n",
      "j-2X9F8BNAZZYIN\n"
     ]
    }
   ],
   "source": [
    "# Don't forget to terminate it when it's not needed\n",
    "!mrjob create-job-flow --num-ec2-instances=1 \\\n",
    "--ec2-instance-type=m1.medium \\\n",
    "--max-hours-idle=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-03ccebffb1b98a81\n",
      "using s3://mrjob-03ccebffb1b98a81/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test.patrickng.20160309.023949.856244\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Copying non-input files into s3://mrjob-03ccebffb1b98a81/tmp/test.patrickng.20160309.023949.856244/files/\n",
      "Adding our job to existing job flow j-2X9F8BNAZZYIN\n",
      "Job launched 32.5s ago, status WAITING: Cluster ready after last step completed. (test.patrickng.20160309.023949.856244: Step 1 of 1)\n",
      "Job launched 64.7s ago, status RUNNING: Running step (test.patrickng.20160309.023949.856244: Step 1 of 1)\n",
      "Job launched 97.1s ago, status RUNNING: Running step (test.patrickng.20160309.023949.856244: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 67.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 180\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 429\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 244\n",
      "    FILE_BYTES_WRITTEN: 160543\n",
      "    HDFS_BYTES_READ: 475\n",
      "    S3_BYTES_READ: 180\n",
      "    S3_BYTES_WRITTEN: 429\n",
      "  Job Counters :\n",
      "    Launched map tasks: 5\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 5\n",
      "    SLOTS_MILLIS_MAPS: 44533\n",
      "    SLOTS_MILLIS_REDUCES: 19692\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 3130\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 111\n",
      "    Map input records: 9\n",
      "    Map output bytes: 440\n",
      "    Map output materialized bytes: 388\n",
      "    Map output records: 9\n",
      "    Physical memory (bytes) snapshot: 1657028608\n",
      "    Reduce input groups: 9\n",
      "    Reduce input records: 9\n",
      "    Reduce output records: 5\n",
      "    Reduce shuffle bytes: 388\n",
      "    SPLIT_RAW_BYTES: 475\n",
      "    Spilled Records: 18\n",
      "    Total committed heap usage (bytes): 1593593856\n",
      "    Virtual memory (bytes) snapshot: 5727428608\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test.patrickng.20160309.023949.856244\n",
      "Removing all files in s3://mrjob-03ccebffb1b98a81/tmp/test.patrickng.20160309.023949.856244/\n"
     ]
    }
   ],
   "source": [
    "!python test.py \\\n",
    "s3://patng323-learn-mrjob/input/test?.data \\\n",
    "-r emr \\\n",
    "--emr-job-flow-id j-2X9F8BNAZZYIN \\\n",
    "--no-output \\\n",
    "--output-dir s3://patng323-learn-mrjob/output/ \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-09 10:41:28          0 _SUCCESS\r\n",
      "2016-03-09 10:41:22        429 part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls patng323-learn-mrjob/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://patng323-learn-mrjob/output/part-00000 to ./part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://patng323-learn-mrjob/output/part-00000 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0\"\t[[\"0000000000000000009\", \"9\", \"18\", \"Chair\"]]\r\n",
      "\"12\"\t[[\"0000000000000000001\", \"1\", \"1\", \"Pencil\"]]\r\n",
      "\"2\"\t[[\"0000000000000000002\", \"2\", \"4\", \"Desk\"]]\r\n",
      "\"4\"\t[[\"0000000000000000010\", \"10\", \"3\", \"Chair\"]]\r\n",
      "\"6\"\t[[\"-9223372036854775799\", \"-9\", \"2\", \"Ball\"], [\"-9223372036854775807\", \"-1\", \"10\", \"Ball\"], [\"-9223372036854775807\", \"-1\", \"6\", \"Lamp\"], [\"0000000000000000008\", \"8\", \"7\", \"Ball\"], [\"0000000000000000199\", \"199\", \"20\", \"Ball\"]]\r\n"
     ]
    }
   ],
   "source": [
    "!cat part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this if you want to clear the output \"folder\" in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "name = \"patng323-learn-mrjob\"\n",
    "\n",
    "bucket = s3.Bucket(name)\n",
    "# Clear all items (including \"folders\") if it exists\n",
    "for object in bucket.objects.all():\n",
    "    if object.key.startswith('output/'):\n",
    "        object.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls patng323-learn-mrjob/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget to terminate the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-03ccebffb1b98a81\n",
      "using s3://mrjob-03ccebffb1b98a81/tmp/ as our scratch dir on S3\n",
      "Terminated job flow j-2X9F8BNAZZYIN\n"
     ]
    }
   ],
   "source": [
    "!mrjob terminate-job-flow j-2X9F8BNAZZYIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the input to mapper has terminating newline or not.\n",
    "Answer: NO\n",
    "\n",
    "### How about passing a list inside a tuple as values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test2.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class test2(MRJob):\n",
    "    def mapper1(self, line_no, line):\n",
    "        fields = line.split(',')\n",
    "        v = [\"a\",\"b\",\"c\"]\n",
    "        yield fields[0], (v, len(fields))\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        items = []\n",
    "        for v in values:\n",
    "            yield key, (\"#\".join(v[0]), v[1])\n",
    "        \n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1,\n",
    "                  reducer=self.reducer1)\n",
    "            ]\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test2.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/0/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/0/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python test2.py --step-num=0 --mapper /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/input_part-00000\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/1/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/1/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python test2.py --step-num=0 --mapper /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/input_part-00001\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/reducer/0/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/reducer/0/mrjob.tar.gz:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/pyspark:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python:/Users/patrickng/Programs/spark-1.5.1-bin-hadoop2.6/python/build::/Library/Python/2.7/site-packages\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/job_local_dir/0/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + /usr/bin/python test2.py --step-num=0 --reducer /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/test2.patrickng.20160210.130421.417554/input_part-00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "('0', ['a#b#c', 4])\n",
      "('2', ['a#b#c', 4])\n",
      "('4', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n",
      "('6', ['a#b#c', 4])\n"
     ]
    }
   ],
   "source": [
    "from test2 import test2\n",
    "mr_job = test2(args=['test.data', '-r', 'local', '--strict-protocols'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    print \"Output:\"\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
